While the demand for advanced LLM-based tools at the academic level is clearly present, there is a significant lack of services and solutions that properly address the needs of university students and educators. This deficit is most pronounced in the processing, organization, and clean-up of lecture materials into high-quality, reliable study resources starting from unstructured sources such as recordings and lecture slides.

Currently, most programs and apps that attempt to utilize large language models (LLMs) for this purpose are merely superficial and naïve wrappers around existing LLM APIs. I believe these approaches are clearly not suited for academic use because they lack the depth of explanation and reasoning, often condensing multiple hours of lecture by a professor into a small number of bullet points; while they do capture the overall aspect of the lectures, they miss in the subtleties, and we all know the devil is in the details. The core issue is that academic content is characterized by a high density of information. LLMs, by default, act as lossy encoders: they shorten and summarize, causing a vital loss of information over long-form content that can be critical for passing exams and effective study. Furthermore, simple repeated prompting does not resolve this issue because it does not permit the reliable verification or structural organization necessary for accurate, comprehensive study material.

This necessity for a better solution became clear to me this past March when I was talking with one of my roommates. He was trying to study for an exam by feeding his slides, documents, and lecture recordings to an LLM. He tried public-facing platforms like ChatGPT and then Gemini, but quickly found they were limited because they cannot be programmed. The intuitive next step is to access these services via the API, feed the source materials, and request a study document or ask questions.

However, even this seemingly logical "naïve" approach fails for multiple reasons, particularly with lecture recordings. Typical lectures span one to three hours. Yet, from my testing—even with advanced models such as Gemini 2.5, Gemini 3, and GPT-5—the models can coherently transcribe audio for only about 30 to 40 minutes before they begin hallucinating (*i.e.*, repeating segments of their response nonsensically). Feeding hours of audio directly is also prohibitively expensive, as audio token pricing is often an order of magnitude higher than text tokens. One might argue that a multimodal model could directly summarize audio without a transcript, but this is also unscalable for multi-hour recordings, degrades with increasing length, and offers none of the granularity, checks, or verifications needed to ensure the consistency necessary for academia. Therefore, I developed a layered algorithm that can produce a cohesive document from slides and recordings together, spanning hundreds of pages and dozens of hours, by carefully processing the materials step by step.

Not only did I implement the algorithm, but I also created a platform called "Bequire" where users can sign up. It is not yet released to the public, but I have included snapshots showing how it looks and works. Currently it is in the testing phase with a very limited number of users having access; if you wish to join the testing, you may write to me at the email address [`giovannigravili112@gmail.com`](mailto:giovannigravili112@gmail.com).

This post is about how the algorithm works, the challenges I faced during implementation, the development of the web platform, and practical lessons about LLMs from my months of testing and experimentation. Let's not get ahead of ourselves; the most important part of this post is the algorithm itself, which underlies everything I will discuss.

### The Core Algorithm

The inputs can be audio files or reference materials such as slides, presentations, or articles, and the system accepts either audio only, reference files only, or both together; behavior varies depending on the combination. The algorithm produces a polished, well-structured document with proper citations where possible, equations, and a hierarchical organization. This constitutes the outline of the algorithm.

Due to current LLM limitations, audio files are split into approximately ten-minute segments so the AI can focus on one segment at a time; each segment’s transcription can be run separately, making the operation massively parallelizable. Because individual segment transcriptions are not polished, a secondary cleaning step is required: batches of two or three segments are processed together by another LLM-based step that cleans grammar, syntax, and logic while preserving the nature of the transcription. The resulting batches, processed in parallel, are then concatenated to form the entire transcript. This process is cost-effective by leveraging the full array of Gemini models, particularly the lower-tier ones; in my experience, larger models do not justify their higher price, since lower-end models still perform well on short segments. One obvious issue is speaker attribution: because segments are not processed sequentially, the algorithm cannot reliably draw boundaries for speakers across segments and may lose context between segments. However, the cleanup operation mitigates these problems sufficiently while keeping costs low enough for the product to be market-viable. The outcome of this first processing stage is a transcription with rough timestamp data for the different audio files.

Once the audio files are correctly transcribed, each individual page of the reference documents—slides, presentations, and so on—is independently and in parallel interpreted by the LLM using the image-understanding modality: each page is converted to an image and then fed to the model along with a prompt requesting its interpretation. The content is organized by explicitly marking each page in subsequent prompts and algorithmic steps, thereby preserving metadata such as page association. This process creates a corpus of source materials composed of the full lecture transcript and all extracted reference-file content. Next, it is essential to determine which pages of the reference materials are actually relevant to the lecture; this step reduces costs and lowers the model’s cognitive load by eliminating the need to discern relevance during interpretation. Specifically, only pages that support or explain concepts discussed in the lecture are selected.

This concludes the preparation of the source materials. The next step is performing a structure analysis, in which, rather than generating the entire study document in one massive prompt, the LLM is first asked to create an outline of the document in sections. For each section it should identify the topics and their relevance in the lecture, indicating how much the professor emphasized and prioritized them; determine appropriate section boundaries; establish a title; state what the section introduces and what it connects to—*i.e.*, what it builds on, what to cite and which topics have already been discussed and therefore should not be repeated. Each section is then provided as a new context to the LLM, which generates the document section by section. This process avoids forcing the LLM to compress information and instead allows expansion of specific parts of the lecture without limits. The algorithm is designed so that we may ask the LLM to generate an arbitrary number of sections and thus expand each individually to obtain the desired level of precision on each topic. After the generation of each section, a verification runs that produces a score indicating the section's adherence to the required content and instructions. If the score does not meet the threshold, the generated section is discarded from the LLM context to prevent context pollution, and the section is regenerated until the threshold is met.

### Technical Difficulties

The output of LLMs, even if it resembles Markdown, must be considered unsanitized because it can be partly invalid. For example, lists may lack proper indentation, and LaTeX formulas embedded within the document may not be parsed correctly due to unsupported LaTeX commands, improperly delimited math environments (missing opening or closing delimiters), or the use of dollar signs for currency that interfere with formula parsing. This situation required creating a self-correcting Markdown parser and a custom citation format that the parser interprets; the citation format must be viewable on the web and ultimately parsed into valid HTML, so my implementation of the parser always produces valid HTML. Although it has not undergone fuzz testing, repeated document generation has revealed no major issues. Another problem LLMs often produce—problematic footnotes—was addressed by introducing the custom citation format. 

### The "Bequire" Platform

![]()
While the demand for advanced LLM-based tools at the academic level is clearly present, there is a significant lack of services and solutions that properly address the needs of university students and educators. This deficit is most pronounced in the processing, organization, and clean-up of lecture materials into high-quality, reliable study resources starting from unstructured sources such as recordings and lecture slides.

Currently, most programs and apps that attempt to utilize Large Language Models (LLMs) for this purpose are merely superficial and naïve wrappers around existing LLM APIs. I believe these approaches are clearly not suited for academic use because they lack the depth of explanation and reasoning, often condensing multiple hours of lecture by a professor into a small number of bullet points; while they do capture the overall aspect of the lectures, they miss in the subtleties, and we all know the devil is in the details. The core issue is that academic content is characterized by a high density of information. LLMs, by default, act as lossy encoders: they shorten and summarize, causing a vital loss of information over long-form content that can be critical for passing exams and effective study. Furthermore, simple repeated prompting does not resolve this issue because it does not permit the reliable verification or structural organization necessary for accurate, comprehensive study material.

This necessity for a better solution became clear to me this past March when I was talking with one of my roommates. He was trying to study for an exam by feeding his slides, documents, and lecture recordings to an LLM. He tried public-facing platforms like ChatGPT and then Gemini, but quickly found they were limited because they cannot be programmed. The intuitive next step is to access these services via the API, feed the source materials, and request a study document or ask questions.

However, even this seemingly logical "naïve" approach fails for multiple reasons, particularly with lecture recordings. Typical lectures span one to three hours. Yet, from my testing—even with advanced models such as Gemini 2.5, Gemini 3, and GPT-5—the models can coherently transcribe audio for only about 30 to 40 minutes before they begin hallucinating (_i.e._, repeating segments of their response nonsensically). Feeding hours of audio directly is also prohibitively expensive, as audio token pricing is often an order of magnitude higher than text tokens. One might argue that a multimodal model could directly summarize audio without a transcript, but this is also unscalable for multi-hour recordings, degrades with increasing length, and offers none of the granularity, checks, or verifications needed to ensure the consistency necessary for academia. Therefore, I developed a layered algorithm that can produce a cohesive document from slides and recordings together, spanning hundreds of pages and dozens of hours, by carefully processing the materials step by step.

Not only did I implement the algorithm, but I also created a platform called "Bequire" where users can sign up. It is not yet released to the public, but I have included snapshots showing how it looks and works. Currently it is in the testing phase with a very limited number of users having access; if you wish to join the testing, you may write to me at the email address [`giovannigravili112@gmail.com`](mailto:giovannigravili112@gmail.com).

This post is about how the algorithm works, the challenges I faced during implementation, the development of the web platform, and practical lessons about LLMs from my months of testing and experimentation. Let's not get ahead of ourselves; the most important part of this post is the algorithm itself, which underlies everything I will discuss.

### Technical Stack and Architecture

Before diving into the algorithm, it is important to understand the technical foundation upon which Bequire is built. The platform consists of a Node.js backend running Express.js v5 with TypeScript, and a Next.js 15 frontend with React 19. All LLM operations—transcription, visual document understanding, document generation, and chat—are powered by Google's Generative AI SDK (Gemini), not OpenAI. This choice was deliberate: Gemini's multimodal capabilities enable direct processing of audio and images without intermediate conversion steps, and its cost structure is more favorable for the high token volumes required by academic document generation.

The backend uses Firebase for authentication and Firestore as the primary database, with Google Cloud Storage for file persistence. Document processing is handled asynchronously via BullMQ, a Redis-backed job queue that manages the multi-stage pipeline. Each document generation job progresses through stages—audio transcription, visual document understanding, page relevance detection, section generation, and PDF compilation—with real-time status updates written to Firestore so the frontend can display progress to the user. This architecture ensures that long-running operations do not block API endpoints and that users receive immediate feedback as their documents are being generated.

The frontend is built with Next.js 15 using Turbopack for fast builds and hot module reloading during development. The UI is constructed from Radix UI primitives styled with TailwindCSS v4, providing accessible components with consistent behavior across browsers. For PDF viewing, the frontend uses react-pdf, which renders PDFs in a canvas element and supports features like page navigation, zoom, and text selection. The chat interface employs Motion (Framer Motion) for smooth animations and transitions, creating a polished user experience that feels responsive even when processing large documents.

### The Core Algorithm

The inputs can be audio files or reference materials such as slides, presentations, or articles, and the system accepts either audio only, reference files only, or both together; behavior varies depending on the combination. The algorithm produces a polished, well-structured document with proper citations where possible, equations, and a hierarchical organization. This constitutes the outline of the algorithm.

Due to current LLM limitations, audio files are split into approximately ten-minute segments so the AI can focus on one segment at a time; each segment's transcription can be run separately, making the operation massively parallelizable. The splitting is performed using FFmpeg with codec detection to ensure the output format matches the input, preserving audio quality while maintaining compatibility with Gemini's supported audio formats. Each segment is uploaded to the model as a binary file, and Gemini's native audio understanding capability generates a transcript directly without requiring intermediate speech-to-text conversion.

Because individual segment transcriptions are not polished, a secondary cleaning step is required: batches of two or three segments are processed together by another LLM-based step that cleans grammar, syntax, and logic while preserving the nature of the transcription. The resulting batches, processed in parallel, are then concatenated to form the entire transcript. This process is cost-effective by leveraging the full array of Gemini models, particularly the lower-tier ones; in my experience, larger models do not justify their higher price, since lower-end models still perform well on short segments. One obvious issue is speaker attribution: because segments are not processed sequentially, the algorithm cannot reliably draw boundaries for speakers across segments and may lose context between segments. However, the cleanup operation mitigates these problems sufficiently while keeping costs low enough for the product to be market-viable. The outcome of this first processing stage is a transcription with rough timestamp data for the different audio files.

Once the audio files are correctly transcribed, each individual page of the reference documents—slides, presentations, and so on—is independently and in parallel interpreted by the LLM using Gemini's native image-understanding modality: each page is converted to an image and then fed to the model along with a prompt requesting its interpretation. The content is organized by explicitly marking each page in subsequent prompts and algorithmic steps, thereby preserving metadata such as page association. This process creates a corpus of source materials composed of the full lecture transcript and all extracted reference-file content.

Next, it is essential to determine which pages of the reference materials are actually relevant to the lecture. This page relevance detection system is one of the most critical components of the algorithm because it directly impacts both cost and document quality. The system runs three parallel LLM queries, each independently analyzing the transcript against the visually extracted page texts to identify which pages discuss concepts mentioned in the lecture. Each run may produce slightly different page ranges due to the stochastic nature of LLM responses, so the algorithm unions the results by merging overlapping and adjacent ranges. Ranges separated by five or fewer pages are also merged under the assumption that intermediate pages likely contain supporting content. This redundant voting mechanism significantly improves recall—ensuring that relevant pages are not missed—while the union operation prevents duplicate work in subsequent processing stages.

This concludes the preparation of the source materials. The next step is performing a structure analysis, in which, rather than generating the entire study document in one massive prompt, the LLM is first asked to create an outline of the document in sections. For each section it should identify the topics and their relevance in the lecture, indicating how much the professor emphasized and prioritized them; determine appropriate section boundaries; establish a title; state what the section introduces and what it connects to—_i.e._, what it builds on, what to cite and which topics have already been discussed and therefore should not be repeated. Each section is then provided as a new context to the LLM, which generates the document section by section. This process avoids forcing the LLM to compress information and instead allows expansion of specific parts of the lecture without limits. The algorithm is designed so that we may ask the LLM to generate an arbitrary number of sections and thus expand each individually to obtain the desired level of precision on each topic. After the generation of each section, a verification runs that produces a score indicating the section's adherence to the required content and instructions. If the score does not meet the threshold, the generated section is discarded from the LLM context to prevent context pollution, and the section is regenerated until the threshold is met.

### Technical Difficulties

The output of LLMs, even if it resembles Markdown, must be considered unsanitized because it can be partly invalid. For example, lists may lack proper indentation, and LaTeX formulas embedded within the document may not be parsed correctly due to unsupported LaTeX commands, improperly delimited math environments (missing opening or closing delimiters), or the use of dollar signs for currency that interfere with formula parsing. This situation required creating a custom Markdown Abstract Syntax Tree (AST) parser that operates in multiple passes to sanitize and reconstruct the document.

The parser handles several common issues: it escapes unescaped dollar signs to prevent currency symbols from being interpreted as math delimiters, unwraps backtick-wrapped LaTeX that the LLM sometimes generates, and converts LaTeX-style delimiters (`\(...\)` and `\[...\]`) to standard Markdown math notation (`$...$` and `$$...$$`). This ensures compatibility with Pandoc's math rendering pipeline.

The parser constructs an AST by tokenizing the Markdown into structural elements—paragraphs, headers, lists, code blocks, tables, and equations—which are then validated and reconstructed into clean Markdown. This approach allows the system to detect and repair common malformations: lists with inconsistent indentation are normalized, unclosed code fences are automatically closed, and tables with misaligned columns are rebalanced. The resulting Markdown is guaranteed to be parsable by standard renderers, eliminating the class of errors that would otherwise break the document display.

For citations, I developed a custom inline format that encodes the description, source filename, and page numbers in a structured way that can be reliably parsed via regex. The LLM is instructed to emit citations in this format whenever referencing material from the uploaded files. During post-processing, the parser extracts all citation markers, parses the page references, and replaces each marker with a superscript footnote number. The footnote data—including the description, filename, and page numbers—is stored separately and displayed in an interactive modal when the user clicks the citation. This approach avoids the traditional footnote format that LLMs struggle to maintain consistently, especially across multi-section documents where footnote numbering can become desynchronized.

Even after extensive quality assurance, documents still exhibit occasional issues such as malformed LaTeX equations and imperfect citations, which highlight the limitations of current LLMs. Many of these problems require improvements to the underlying technology. Despite these shortcomings, independently conducted assessments by multiple testers on my platform found the generated documents to be incredibly helpful in preparing for exams across disciplines including chemistry, statistics, physics, economics, and medicine. Small formatting inconsistencies and other minor errors underscore the unpredictable nature of LLMs and indicate they should not be fully relied upon; such tools should be regarded only as study aids, not as replacements of existing study methods.

### PDF Generation and Multilingual Support

The final stage of document processing is PDF generation, which presents unique challenges due to the need to support arbitrary Unicode content, complex mathematical notation, and multiple writing systems. The system uses Pandoc to convert the document's HTML representation to PDF, with Tectonic as the PDF engine. Tectonic is a modern, self-contained TeX implementation that automatically downloads required packages and fonts, eliminating the dependency management issues typical of traditional LaTeX distributions.

The conversion pipeline works as follows: the cleaned Markdown is first converted to HTML using Pandoc with MathML for math rendering, then the HTML is passed back to Pandoc with a custom LaTeX template for PDF generation. This template includes conditional logic to select appropriate fonts based on the document language. For Chinese, Japanese, and Korean (CJK) documents, the template loads Noto Serif CJK fonts with automatic bold font synthesis via the `CJKoptions` parameter. For Arabic, Hebrew, Thai, and other scripts, language-specific Noto Serif variants are loaded. This ensures that generated PDFs display correctly regardless of the source material's language, which is essential for academic content that often mixes multiple scripts (e.g., English text with Greek symbols, Arabic citations, or Japanese terminology).

The custom LaTeX template also includes structured metadata fields for reference files and audio files, which are displayed on the title page along with creation dates formatted according to the document's locale. Page ranges for reference files and durations for audio files are automatically computed and included. Pandoc is configured with `--pdf-engine-opt=-Zcontinue-on-errors` to force compilation even when encountering recoverable LaTeX errors, preventing minor formatting issues from blocking PDF generation entirely.

### Chat System with Full Document Context

The chat system allows users to query generated documents by including their complete content directly in the conversation context sent to Gemini. Unlike traditional Retrieval-Augmented Generation (RAG) systems that use vector embeddings and semantic search to retrieve relevant chunks, Bequire's approach is simpler and more direct: when a user adds documents to the chat, their entire content—including the generated plaintext, visually extracted reference pages, and transcript—is inserted into the conversation history as special "document" messages.

This design choice prioritizes accuracy over efficiency. By providing the model with the full document rather than retrieved excerpts, there is no risk of missing relevant information due to imperfect retrieval. The tradeoff is that the system can only handle documents that fit within Gemini's context window, but for academic documents generated by the platform, this limitation is rarely reached. Each document message is structured with clear headers and page markers to help the model navigate the content and cite specific pages when responding.

One feature unique to Bequire's chat system is the message autocorrect capability. Before sending a user's message to the LLM, the system can optionally run it through a corrective prompt that fixes grammar, spelling, and syntax errors while preserving the user's intent. This is particularly useful when typing complex questions quickly or when working in a non-native language. The corrected message is displayed to the user with visual feedback before being sent to the main chat model, allowing them to verify that the correction preserved their intended meaning.

The chat backend maintains conversation history in Firestore and tracks which documents are currently in context. When a new message is sent, the system builds a complete chat history by converting system prompts, document content, user messages, and model responses into the format expected by Gemini's chat API. The model is instructed to respond according to a selected style—balanced, concise, or guided learning—and to cite sources using the same custom citation format employed during document generation. Citations in the response are parsed and linked to the corresponding pages in the PDF viewer, allowing users to immediately verify the model's claims against the source material.

### The "Bequire" Platform

Over the past months I developed a web platform that enables users to run the algorithm; it is responsive across all screen sizes and includes accessibility options. After signing up, a user can create projects, which are defined as collections of documents produced by the algorithm. A project functions not just as a folder but as a scoped grouping: users can gather documents from different lectures belonging to the same class and query all those documents simultaneously through a chat interface. The list of projects is presented on what I call the library page.

![](Screenshot 2025-12-11 at 12.05.20.png)

Once a user selects a project, they are given the documents within it and access to the chat interface for all documents in that project.

![](Screenshot 2025-12-11 at 12.06.05.png)

Documents can be created via the "New Document" button on the top bar. Users can upload audio and reference files, choose the order of the audio files, and in the future may be able to assign importance to reference files to prioritize them. Each document can have one of three lengths—short, medium, or long. Length varies according to source materials: a long document typically corresponds to about 20 PDF pages, a medium document is around 14–15 pages, and a short document ranges from 5 to 10 pages.

![](Screenshot 2025-12-11 at 12.06.18.png)

Each document includes metadata so the user can differentiate among the various created documents. For each document a title and description are generated automatically. Documents themselves are immutable and, in the current version, can only be downloaded as a PDF in addition to being accessible on the platform itself.

On the right, the chat panel can be collapsed or revealed via a toggle. The chat input at the bottom contains multiple utilities, including a view of which documents are currently in the context, creation of a new chat, and an autocorrect feature for messages that facilitates input of formulas via plain text. This feature not only corrects formulas but also grammar and syntax. The user can choose a response style that is balanced, concise, or oriented toward guided learning, allowing you to focus on the desired experience.

![](Screenshot 2025-12-11 at 12.07.04.png)

Once the document is accessed, it displays an interactive table of contents at the top and the chat panel associated with the project on the right. The document is automatically added to the chat context and can be queried immediately.

![](Screenshot 2025-12-11 at 12.07.42.png)

The document, as previously noted, can be downloaded in PDF form; for those curious, I have uploaded it to Google Drive and it is available at the following [link](https://drive.google.com/file/d/1AchKmkigQVG6NGr7cOH0qRRjYY5eLcn9/view?usp=sharing).

Each document also includes a transcript; this cleaned, and roughly time-stamped version is the one used by the LLM in creating the document.

![](Screenshot 2025-12-11 at 12.07.46.png)

By scrolling down, the rest of the document can be read. If reference files have been provided, as in this case, citations will be present. These appear as footnotes in the PDF at the bottom of each page, but as clickable popups in the document within the website. Clicking a citation reveals the cited content, the referenced document, and the page where the content appears. Pressing the "Open the Reference File" button opens an inline view of the PDF at the specified page.

![](Screenshot 2025-12-11 at 12.08.51.png)

By scrolling further down, the user can see what the LLM has understood about each page; this comprehension is provided without any other context but may help the reader identify faults in the LLM's interpretation. Adding another verifiable step in the process can clarify misunderstandings: if something has not been properly understood, the user can press the "Ask about Page" button to pose a direct question to the LLM. Visual content is typically well interpreted by the LLM, and questions about diagrams or complex images can also be asked. Users can query entire pages or specific sections within the document by pressing the "Ask about Section" button next to each section header, making the website a superior medium for studying documents while still allowing accessibility through PDF download.

![](Screenshot 2025-12-11 at 12.09.46.png)

Focusing specifically on the last image above, a question was asked about the given page and the LLM responded by citing a specific piece of information from that page. This can occur for any general question that allows tracing, or approximate tracing, of information across multiple documents. By clicking the button in the pop-up, you can open the PDF reader and view the LLM’s understanding of each page in full-screen.

![](Screenshot 2025-12-11 at 12.09.58.png)

The user can freely navigate between the different pages, directly bypassing the document if desired and focusing specifically on the reference files they provided. The website is available in both light and full dark themes; the PDF files have the same dark-mode option to improve readability during night study sessions and reduce eye strain.

![](Screenshot 2025-12-11 at 12.10.16.png)

Lastly, here is another screenshot of the website in dark theme. In the top right, a queue shows documents being created that are accessible to users, and progress is displayed dynamically. A menu is available for users to choose different options, including zooming, selecting a theme, creating a new chat, printing the document, playing music, and signing out; users can play study or relaxing music freely available from YouTube directly within the website.

![](Screenshot 2025-12-11 at 12.41.38.png)
